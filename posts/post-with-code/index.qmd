---
title: "Creating my first end-to-end data science pipeline: lesson learned "
author: "Daria Khon"
date: "2025-01-15"
categories: [docker, teamwork, pipeline]
image: "image.jpeg"
---

Few years ago I was at school, doing my undergrad in chemical engineering and studying pipe design every day. Years passed, my career has changed and now I find myself once again intrigued by the pipelines, only this time the more elegant ones: data science pipelines.

>A data science pipeline is an end-to-end process of gathering raw data, transforming it into a format suitable for analysis, feeding it into a model, and obtaining results to answer specific questions. 

After the workflow has been constructed, it is also put into a **reproducibility wrapper**, which means automating the steps in the pipeline by using technologies like Docker containerization. If you had asked me about the data science workflow a few months ago, I would have stopped at the first part, the steps. However, as I learned with my recent project experience, the real challenge lies in achieving reproducibility through containerization.

As such I want to share my two important lessons learned, and hope they can be helpful to anyone else who is starting on the data science journey.

### Two key lessons from this journey that I will go over in this blog:
1. Learning about Docker and using it for the first time. <br>
2. Recognizing that any team can excel when hinged on a fostering strong professional relationships.

### But first, a little detour to provide context on the project itself:

My team and I built a predictive model for classifying wine color, red vs white, based on its physicochemical properties, such as acidity, pH, alcohol content, etc. We chose logistic regression for its simplicity, interpretability, and suitability for binary classification tasks. In this project, we build a reproducible pipeline that consisted of the following steps:
<ul>
<li>Collecting data from UCI Machine Learning Repository</li>
<li>Cleaning and validating data</li>
<li>Hyperparameter C optimization for Logistic Regression on F1-score (balancing out precision and recall)</li>
<li>Results interpretation via confusion matrix, and Precision-Recall curve</li>
<li>Incorporating unit tests for all our scripts</li>
<li>Report generation with our methods, results and discussion</li>
<li>Creating a virtual conda environment and a docker container for reproducibility</li>
</ul>
**List of data science tools used:** Scikit-learn, Docker, pytest, git <br>
Check out our [github repo](https://github.com/UBC-MDS/DSCI522-2425-22-wine-chromatic-profile/tree/main) for more details.

### Lesson 1: Learning and using Docker

The key learning of completing this project was that no matter how well you design your model and code it, if a new person cannot reproduce it on their own, and corroborate your findings, all your effort goes void. The package versions, the operating system, and other factors all come into play when it comes to reproducibility, and luckily for us there is a way to lock it all in, wrap and send it to whoever you like - Docker. 

> Docker is a software platform to create, run and manage containers. Containers are something like a virtual machine, in a sense that it has its operating system. However, unlike virtual machines, containers don’t simulate the entire computer, but rather create a sandboxed environment that behaves like a virtual machine. 

Docker containers are created from Docker **“images”**, which are blueprints of an application. These images are immutable once created and contain everything needed to run the application. To build a Docker image, you write instructions in a **Dockerfile**, a plain text file with no extension that is always named Dockerfile.

**Below is our docker file that created an image for running our analysis in a Jupyter Lab**
<img src="dockerfile.png" style="width:60%;">

**Let’s break down what each line here means:**

`FROM quay.io/jupyter/minimal-notebook:afe30f0c9ad8`
This line sets a base image for docker build: it ensures that the environment starts with a minimal Jupyter Notebook setup.

`COPY conda-linux-64.lock /tmp/conda-linux-64.lock`
`COPY requirements.txt /tmp/requirements.txt`
These lines install dependencies into the Docker image via local conda lock and requirements.txt files that contain specific package versions. Check out this article for more information about conda-lock files.

`USER root` followed by `USER $NB_UID`
Switches user to `root`, which is required for installation of system level packages. And then switches it back to default non-root user once the required steps are completed.

`RUN` commands are then for actually installing system packages, dependencies, etc. that were mentioned above.

Looks simple enough, but if you are still confused about the process and what everything means, here is my approach for learning docker, which frankly just involves a lot of practice:
<ol>  
1. Read this [Medium article](https://medium.com/@JeffyJeff/the-beginners-guide-to-docker-fa4c4d3181e7) by Jean F Beaulieu
2. Follow this [Youtube tutorial](https://www.youtube.com/watch?v=gAkwW2tuIqE) to build your own toy Docker image   
3. Experiment: try building a Dockerfile for a small project of your own, something you would be invested in, perhahps related to your hobby?  
4. Finally, it helped me to think about the Docker file as **layers of cake**, or any other metaphore of sequenced build up you want to think about. The idea is that each command contributes a distinct layer, ultimately creating a complete and reproducible “recipe” for your project:</ol>
<ul><ul>
<li>`FROM` command serves as the sponge cake base - the foundation on which everything else is built. It comes pre-baked (like the Jupyter Notebook image) with essential ingredients to ensure stability and consistency.  </li>  
<li>`COPY` command serves as the filling and represents the critical flavors (dependencies) that make the cake unique. By copying these files into the container, we ensure the right ingredients are ready to blend into the final mix.</li> 
<li>`USER` switches are like switching between a master chef (root) and an assistant baker (default user). The master chef handles delicate tasks like preparing the tools (installing system-level packages), then hands over to the assistant for day-to-day operations.</li> 
<li>Once everything is assembled you are ready to run away with the `RUN` command (pun intended). These commands are like instructions on how to assemble the cake and how to bake it, so that a reproducible results is ready to be served.</li> 
</ul></ul>
<img src="cake.png" style="width:100%;">

By mastering Docker, you will not only streamline your workflows but also ensure your projects are sharable and reproducible. This is what will elevate a project from amateur to professional quality.

### Lesson 2: Invest in your team culture, if you want good teammates

I have several years of project management experience, and one key principle I bring to every new team is the importance of coming together on day one to clearly define the goal, the time and effort we are willing to invest, and, most importantly, our preferred communication methods and frequency. It may sound trivial, but it’s often overlooked, especially in STEM fields where individuals are eager to take on developer roles but not always to foster effective teamwork.

My team followed this approach from the start, and it made all the difference. Without it, we might not have found our rhythm, and the experience could have been very different. While I am not claiming this approach guarantees perfect collaboration, I can confidently say that since adopting it, I have yet to experience a truly challenging team dynamic.

Thank you for stopping by, and hope you found some points of resonance while reading this!


[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, welcome to my blog! My name is Daria, I am currently pursuing Masters in Data Science at the University of British Columbia. My professional interests lie at the intersection of data and biotechnology, while my personal interests involve things like reading, skiing and playing volleyball."
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About",
    "section": "",
    "text": "Hello, welcome to my blog! My name is Daria, I am currently pursuing Masters in Data Science at the University of British Columbia. My professional interests lie at the intersection of data and biotechnology, while my personal interests involve things like reading, skiing and playing volleyball."
  },
  {
    "objectID": "docs/posts/post-with-code/index.html",
    "href": "docs/posts/post-with-code/index.html",
    "title": "Creating my first end-to-end data science pipeline: lesson learned",
    "section": "",
    "text": "Few years ago I was at school, doing my undergrad in chemical engineering and studying pipe design every day. Years passed, my career has changed and now I find myself once again intrigued by the pipelines, only this time the more elegant ones: data science pipelines.\n\nA data science pipeline is an end-to-end process of gathering raw data, transforming it into a format suitable for analysis, feeding it into a model, and obtaining results to answer specific questions.\n\nAfter the workflow has been constructed, it is also put into a reproducibility wrapper, which means automating the steps in the pipeline by using technologies like Docker containerization. If you had asked me about the data science workflow a few months ago, I would have stopped at the first part, the steps. However, as I learned with my recent project experience, the real challenge lies in achieving reproducibility through containerization.\nAs such I want to share my two important lessons learned, and hope they can be helpful to anyone else who is starting on the data science journey.\n\nTwo key lessons stood out from this journey that I will go over in this blog:\n\nLearning about Docker and using it for the first time. \nRecognizing that any team can excel when hinged on a fostering strong professional relationships.\n\n\n\nBut first, a little detour to provide a context on the project itself:\nMy team and I built a predictive model for classifying wine color, red vs white, based on its physicochemical properties, such as acidity, pH, alcohol content, etc. We chose logistic regression for its simplicity, interpretability, and suitability for binary classification tasks. In this project, we build a reproducible pipeline that consisted of the following steps:\n\n\nCollecting data from UCI Machine Learning Repository\n\n\nCleaning and validating data\n\n\nHyperparameter C optimization for Logistic Regression on F1-score (balancing out precision and recall)\n\n\nResults interpretation via confusion matrix, and Precision-Recall curve\n\n\nIncorporating unit tests for all our scripts\n\n\nReport generation with our methods, results and discussion\n\n\nCreating a virtual conda environment and a docker container for reproducibility\n\n\nList of data science tools used: Scikit-learn, Docker, pytest, git  Check out our github repo for more details.\n\n\nLesson 1: Learning and using Docker\nThe key learning of completing this project was that no matter how well you design your model and code it, if a new person cannot reproduce it on their own, and corroborate your findings, all your effort goes void. The package versions, the operating system, and other factors all come into play when it comes to reproducibility, and luckily for us there is a way to lock it all in, wrap and send it to whoever you like - Docker.\n\nDocker is a software platform to create, run and manage containers. Containers are something like a virtual machine, in a sense that it has its operating system. However, unlike virtual machines, containers don’t simulate the entire computer, but rather create a sandboxed environment that behaves like a virtual machine.\n\nDocker containers are created from Docker “images”, which are blueprints of an application. These images are immutable once created and contain everything needed to run the application. To build a Docker image, you write instructions in a Dockerfile, a plain text file with no extension that is always named Dockerfile.\nHere is an example of our docker file that created an image for running our analysis in a Jupyter Lab \nLet’s break down what each line here means:\nFROM quay.io/jupyter/minimal-notebook:afe30f0c9ad8 This line sets a base image for docker build: it ensures that the environment starts with a minimal Jupyter Notebook setup.\nCOPY conda-linux-64.lock /tmp/conda-linux-64.lock COPY requirements.txt /tmp/requirements.txt These lines install dependencies into the Docker image via local conda lock and requirements.txt files that contain specific package versions. Check out this article for more information about conda-lock files.\nUSER root followed by USER $NB_UID Switches user to root, which is required for installation of system level packages. And then switches it back to default non-root user once the required steps are completed.\nRUN commands are then for actually installing system packages, dependencies, etc. that were mentioned above.\nLooks simple enough, but if you are still confused about the process and what everything means, here is my approach for learning docker, which frankly just involves a lot of practice:\n\n\nRead this Medium article by Jean F Beaulieu\nFollow this Youtube tutorial to build your own toy Docker image\n\nExperiment: try building a Dockerfile for a small project of your own, something you would be invested in, perhahps related to your hobby?\n\nFinally, it helped me to think about the Docker file as layers of cake, or any other metaphore of sequenced build up you want to think about. The idea is that each command contributes a distinct layer, ultimately creating a complete and reproducible “recipe” for your project:\n\n\n\n\nFROM command serves as the sponge cake base - the foundation on which everything else is built. It comes pre-baked (like the Jupyter Notebook image) with essential ingredients to ensure stability and consistency.\n\n\nCOPY command serves as the filling and represents the critical flavors (dependencies) that make the cake unique. By copying these files into the container, we ensure the right ingredients are ready to blend into the final mix.\n\n\nUSER switches are like switching between a master chef (root) and an assistant baker (default user). The master chef handles delicate tasks like preparing the tools (installing system-level packages), then hands over to the assistant for day-to-day operations.\n\n\nOnce everything is assembled you are ready to run away with the RUN command (pun intended). These commands are like instructions on how to assemble the cake and how to bake it, so that a reproducible results is ready to be served.\n\n\n\n\n\nBy mastering Docker, you will not only streamline your workflows but also ensure your projects are sharable and reproducible. This is what will elevate a project from amateur to professional quality.\n\n\nLesson 2: Invest in your team culture, if you want good teammates\nI have several years of project management experience, and one key principle I bring to every new team is the importance of coming together on day one to clearly define the goal, the time and effort we are willing to invest, and, most importantly, our preferred communication methods and frequency. It may sound trivial, but it’s often overlooked, especially in STEM fields where individuals are eager to take on developer roles but not always to foster effective teamwork.\nMy team followed this approach from the start, and it made all the difference. Without it, we might not have found our rhythm, and the experience could have been very different. While I am not claiming this approach guarantees perfect collaboration, I can confidently say that since adopting it, I have yet to experience a truly challenging team dynamic.\nThank you for stopping by, and hope you found some points of resonance while reading this!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Creating my first end-to-end data science pipeline: lesson learned",
    "section": "",
    "text": "Few years ago I was at school, doing my undergrad in chemical engineering and studying pipe design every day. Years passed, my career has changed and now I find myself once again intrigued by the pipelines, only this time the more elegant ones: data science pipelines.\n\nA data science pipeline is an end-to-end process of gathering raw data, transforming it into a format suitable for analysis, feeding it into a model, and obtaining results to answer specific questions.\n\nAfter the workflow has been constructed, it is also put into a reproducibility wrapper, which means automating the steps in the pipeline by using technologies like Docker containerization. If you had asked me about the data science workflow a few months ago, I would have stopped at the first part, the steps. However, as I learned with my recent project experience, the real challenge lies in achieving reproducibility through containerization.\nAs such I want to share my two important lessons learned, and hope they can be helpful to anyone else who is starting on the data science journey.\n\nTwo key lessons stood out from this journey that I will go over in this blog:\n\nLearning about Docker and using it for the first time. \nRecognizing that any team can excel when hinged on a fostering strong professional relationships.\n\n\n\nBut first, a little detour to provide a context on the project itself:\nMy team and I built a predictive model for classifying wine color, red vs white, based on its physicochemical properties, such as acidity, pH, alcohol content, etc. We chose logistic regression for its simplicity, interpretability, and suitability for binary classification tasks. In this project, we build a reproducible pipeline that consisted of the following steps:\n\n\nCollecting data from UCI Machine Learning Repository\n\n\nCleaning and validating data\n\n\nHyperparameter C optimization for Logistic Regression on F1-score (balancing out precision and recall)\n\n\nResults interpretation via confusion matrix, and Precision-Recall curve\n\n\nIncorporating unit tests for all our scripts\n\n\nReport generation with our methods, results and discussion\n\n\nCreating a virtual conda environment and a docker container for reproducibility\n\n\nList of data science tools used: Scikit-learn, Docker, pytest, git  Check out our github repo for more details.\n\n\nLesson 1: Learning and using Docker\nThe key learning of completing this project was that no matter how well you design your model and code it, if a new person cannot reproduce it on their own, and corroborate your findings, all your effort goes void. The package versions, the operating system, and other factors all come into play when it comes to reproducibility, and luckily for us there is a way to lock it all in, wrap and send it to whoever you like - Docker.\n\nDocker is a software platform to create, run and manage containers. Containers are something like a virtual machine, in a sense that it has its operating system. However, unlike virtual machines, containers don’t simulate the entire computer, but rather create a sandboxed environment that behaves like a virtual machine.\n\nDocker containers are created from Docker “images”, which are blueprints of an application. These images are immutable once created and contain everything needed to run the application. To build a Docker image, you write instructions in a Dockerfile, a plain text file with no extension that is always named Dockerfile.\nHere is an example of our docker file that created an image for running our analysis in a Jupyter Lab \nLet’s break down what each line here means:\nFROM quay.io/jupyter/minimal-notebook:afe30f0c9ad8 This line sets a base image for docker build: it ensures that the environment starts with a minimal Jupyter Notebook setup.\nCOPY conda-linux-64.lock /tmp/conda-linux-64.lock COPY requirements.txt /tmp/requirements.txt These lines install dependencies into the Docker image via local conda lock and requirements.txt files that contain specific package versions. Check out this article for more information about conda-lock files.\nUSER root followed by USER $NB_UID Switches user to root, which is required for installation of system level packages. And then switches it back to default non-root user once the required steps are completed.\nRUN commands are then for actually installing system packages, dependencies, etc. that were mentioned above.\nLooks simple enough, but if you are still confused about the process and what everything means, here is my approach for learning docker, which frankly just involves a lot of practice:\n\n\nRead this Medium article by Jean F Beaulieu\nFollow this Youtube tutorial to build your own toy Docker image\n\nExperiment: try building a Dockerfile for a small project of your own, something you would be invested in, perhahps related to your hobby?\n\nFinally, it helped me to think about the Docker file as layers of cake, or any other metaphore of sequenced build up you want to think about. The idea is that each command contributes a distinct layer, ultimately creating a complete and reproducible “recipe” for your project:\n\n\n\n\nFROM command serves as the sponge cake base - the foundation on which everything else is built. It comes pre-baked (like the Jupyter Notebook image) with essential ingredients to ensure stability and consistency.\n\n\nCOPY command serves as the filling and represents the critical flavors (dependencies) that make the cake unique. By copying these files into the container, we ensure the right ingredients are ready to blend into the final mix.\n\n\nUSER switches are like switching between a master chef (root) and an assistant baker (default user). The master chef handles delicate tasks like preparing the tools (installing system-level packages), then hands over to the assistant for day-to-day operations.\n\n\nOnce everything is assembled you are ready to run away with the RUN command (pun intended). These commands are like instructions on how to assemble the cake and how to bake it, so that a reproducible results is ready to be served.\n\n\n\n\n\nBy mastering Docker, you will not only streamline your workflows but also ensure your projects are sharable and reproducible. This is what will elevate a project from amateur to professional quality.\n\n\nLesson 2: Invest in your team culture, if you want good teammates\nI have several years of project management experience, and one key principle I bring to every new team is the importance of coming together on day one to clearly define the goal, the time and effort we are willing to invest, and, most importantly, our preferred communication methods and frequency. It may sound trivial, but it’s often overlooked, especially in STEM fields where individuals are eager to take on developer roles but not always to foster effective teamwork.\nMy team followed this approach from the start, and it made all the difference. Without it, we might not have found our rhythm, and the experience could have been very different. While I am not claiming this approach guarantees perfect collaboration, I can confidently say that since adopting it, I have yet to experience a truly challenging team dynamic.\nThank you for stopping by, and hope you found some points of resonance while reading this!"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Creating my first end-to-end data science pipeline: lesson learned\n\n\n\n\n\n\ndocker\n\n\nteamwork\n\n\npipeline\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nDaria Khon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Creating my first end-to-end data science pipeline: lesson learned\n\n\n\n\n\n\ndocker\n\n\nteamwork\n\n\npipeline\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nDaria Khon\n\n\n\n\n\n\n\n\n\n\n\n\nCreating my first end-to-end data science pipeline: lesson learned\n\n\n\n\n\n\ndocker\n\n\nteamwork\n\n\npipeline\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nDaria Khon\n\n\n\n\n\n\nNo matching items"
  }
]